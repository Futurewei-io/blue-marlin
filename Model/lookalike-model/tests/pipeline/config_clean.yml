product_tag: 'lookalike'
pipeline_tag: 'unittest'
persona_table_name: 'lookalike_unittest_clean_input_persona'
showlog_table_name: 'lookalike_unittest_clean_input_showlog'
clicklog_table_name: 'lookalike_unittest_clean_input_clicklog'
keywords_table: 'lookalike_unittest_clean_input_keywords'
log:
  level: 'ERROR' # log level for spark and app
pipeline:
  main_clean:  
    did_bucket_num: 2 # Number of partitions for did
    load_logs_in_minutes: 1440 #1440/day, original=14400
    create_keywords: False # set True for first run, then keep False to use the created table.
    persona_output_table: 'lookalike_unittest_clean_output_persona'
    showlog_output_table: 'lookalike_unittest_clean_output_showlog'
    clicklog_output_table: 'lookalike_unittest_clean_output_clicklog'
    conditions: {
      'new_slot_id_list': [
          'abcdef0', 'abcdef1', 'abcdef2', 'abcdef3', 'abcdef4', 
          'abcdef5', 'abcdef6', 'abcdef7', 'abcdef8', 'abcdef9'
      ],
      'new_slot_id_app_name_list': [
          'Huawei Magazine', 'Huawei Browser', 'Huawei Video', 'Huawei Music', 'Huawei Reading', 
          'Huawei Magazine', 'Huawei Browser', 'Huawei Video', 'Huawei Music', 'Huawei Reading'
      ],
      'starting_date': '2020-01-01',
      'ending_date': '2020-01-11'
    }
  main_logs:
    interval_time_in_seconds: 86400 # default=1 day, group logs in interval time.
    logs_output_table_name: '{product_tag}_{pipeline_tag}_logs'
  main_trainready:
    trainready_output_table: '{product_tag}_{pipeline_tag}_trainready'
  tfrecords:
    tfrecords_statistics_path: '{product_tag}_{pipeline_tag}_tfrecord_statistics.pkl'
    tfrecords_hdfs_path: '{product_tag}_{pipeline_tag}_tfrecord' # it is hdfs location for tfrecords, over-writes the existing files
  cutting_date: 1584748800
  length: 10

